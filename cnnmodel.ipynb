{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate= SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, dataset, transform, target_sample_rate, num_samples, device):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.device = device\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_name = os.path.join(self.data_dir, self.dataset.iloc[idx, 0])\n",
    "        label = self.dataset.iloc[idx, 1]\n",
    "        signal, sr = torchaudio.load(audio_name)\n",
    "        signal = signal.to(device)\n",
    "        signal = self._resample(signal, sr)\n",
    "        signal = self.cut_signal(signal)\n",
    "        signal = self._right_pad(signal)\n",
    "        signal = self.transform(signal)\n",
    "        return label, signal\n",
    "    \n",
    "    def _right_pad(self, signal):\n",
    "        if signal.shape[1] < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "    \n",
    "    def cut_signal(self, signal):\n",
    "        if signal.shape[1]> self.num_samples:\n",
    "            signal = signal[:, : self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def _resample(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_dir = 'C:/Users/hp/OneDrive/Desktop/AudioClassification/TrainAudioFiles'\n",
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = MyDataset(train_dir, train_df, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES, device)\n",
    "TestDataset = MyDataset(train_dir, test_df, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive\\Desktop\\AudioClassification\\myenv\\lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'AudioMetaData' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m audio_formats\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;66;03m# Specify the directory containing your audio files\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m audio_formats \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_audio_formats\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrainAudioFiles\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio formats found:\u001b[39m\u001b[38;5;124m\"\u001b[39m, audio_formats)\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mcheck_audio_formats\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      8\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n\u001b[0;32m      9\u001b[0m         info \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39minfo(file_path)\n\u001b[1;32m---> 10\u001b[0m         audio_format \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m         audio_formats\u001b[38;5;241m.\u001b[39madd(audio_format)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio_formats\n",
      "\u001b[1;31mTypeError\u001b[0m: 'AudioMetaData' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "\n",
    "def check_audio_formats(directory):\n",
    "    audio_formats = set()  # Use a set to store unique audio formats\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.mp3'):  # Adjust the extension based on your audio format\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            info = torchaudio.info(file_path)\n",
    "            audio_format = info[0].get(\"format\", \"unknown\")\n",
    "            audio_formats.add(audio_format)\n",
    "    return audio_formats\n",
    "\n",
    "  # Specify the directory containing your audio files\n",
    "audio_formats = check_audio_formats('TrainAudioFiles')\n",
    "print(\"Audio formats found:\", audio_formats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4652 sample in dataset\n",
      "neutral tensor([[[1.2748e-03, 1.5606e-03, 1.3529e-04,  ..., 5.0757e-03,\n",
      "          3.5216e-03, 4.0229e-04],\n",
      "         [7.3054e-02, 2.8878e-02, 1.0325e-02,  ..., 2.1807e-02,\n",
      "          4.4059e-02, 1.1529e-03],\n",
      "         [1.1629e-01, 1.3923e-01, 2.1254e-02,  ..., 1.1388e-01,\n",
      "          3.8099e-02, 1.1796e-02],\n",
      "         ...,\n",
      "         [7.6484e-05, 1.1332e-04, 7.6166e-05,  ..., 1.3318e-02,\n",
      "          2.0225e-03, 9.4653e-04],\n",
      "         [7.8815e-05, 6.8507e-05, 5.0665e-05,  ..., 1.9693e-02,\n",
      "          2.2802e-03, 3.7280e-04],\n",
      "         [1.2547e-04, 8.8900e-05, 5.0916e-05,  ..., 2.6656e-02,\n",
      "          1.5232e-03, 2.3198e-04]],\n",
      "\n",
      "        [[2.3210e-03, 1.8789e-03, 2.1479e-04,  ..., 6.0703e-03,\n",
      "          2.4248e-03, 1.1396e-04],\n",
      "         [7.2223e-02, 2.6976e-02, 1.1857e-02,  ..., 2.2719e-02,\n",
      "          4.2457e-02, 1.2342e-03],\n",
      "         [1.2092e-01, 1.3189e-01, 2.1706e-02,  ..., 1.2151e-01,\n",
      "          3.7379e-02, 1.2039e-02],\n",
      "         ...,\n",
      "         [1.2171e-04, 1.2420e-04, 1.0712e-04,  ..., 1.2891e-02,\n",
      "          1.5995e-03, 8.8818e-04],\n",
      "         [1.4423e-04, 8.1088e-05, 5.2376e-05,  ..., 1.9692e-02,\n",
      "          1.9402e-03, 3.5802e-04],\n",
      "         [1.7994e-04, 7.5046e-05, 8.7347e-05,  ..., 2.6124e-02,\n",
      "          1.4936e-03, 2.1793e-04]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(TrainDataset)} sample in dataset\")\n",
    "signal, label = TrainDataset[0]\n",
    "print(signal, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 1,\n",
    "                      out_channels= 16,\n",
    "                      padding= 2,\n",
    "                      stride= 1,\n",
    "                      kernel_size= 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 16,\n",
    "                      out_channels= 32,\n",
    "                      padding= 2,\n",
    "                      stride= 1,\n",
    "                      kernel_size= 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 32,\n",
    "                      out_channels= 64,\n",
    "                      padding= 2,\n",
    "                      stride= 1,\n",
    "                      kernel_size= 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64* 5 * 4, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_layer(x)\n",
    "        x = self.conv2_layer(x)\n",
    "        x = self.conv3_layer(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "train_loader = DataLoader(TrainDataset,\n",
    "                          batch_size= BATCH_SIZE,\n",
    "                          num_workers= NUM_WORKERS,\n",
    "                          shuffle= True)\n",
    "test_loader = DataLoader(TestDataset,\n",
    "                          batch_size= BATCH_SIZE,\n",
    "                          num_workers= NUM_WORKERS,\n",
    "                          shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = [\n",
    "    \"neutral\",\n",
    "    \"joy\",\n",
    "    \"disgust\",\n",
    "    \"surprise\",\n",
    "    \"sadness\",\n",
    "    \"fear\",\n",
    "    \"anger\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "    for input, target in tqdm(data_loader, desc=\"Training\", leave=False):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/291 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "cnn = Classification().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(cnn.parameters(),\n",
    "                                lr=LEARNING_RATE)\n",
    "\n",
    "# train model\n",
    "train(cnn, train_loader, loss_fn, optimiser, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n",
    "        predicted_index = predictions[0].argmax(0)\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "    return predicted, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = TrainDataset[0][0], TrainDataset[0][1] # [batch size, num_channels, fr, time]\n",
    "input.unsqueeze_(0)\n",
    "\n",
    "# make an inference\n",
    "predicted, expected = predict(cnn, input, target,\n",
    "                                class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
